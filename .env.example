# TF-AI Environment Configuration
# Copy this file to .env and fill in the values for your chosen provider.
# Never commit .env to source control.

# ── Model Provider ────────────────────────────────────────────────────────────
# Select the inference backend: ollama | openai | azure | bedrock | gemini
MODEL_PROVIDER=ollama

# Model name or deployment ID
MODEL_NAME=llama3

# Max tokens per response (default: 4096)
MODEL_MAX_TOKENS=4096

# Temperature 0.0–1.0 (default: 0.2 — low for deterministic code generation)
MODEL_TEMPERATURE=0.2

# ── Ollama (local) ────────────────────────────────────────────────────────────
# Only required if MODEL_PROVIDER=ollama
MODEL_BASE_URL=http://localhost:11434

# ── OpenAI ────────────────────────────────────────────────────────────────────
# Only required if MODEL_PROVIDER=openai
# Native SDK env var: OPENAI_API_KEY=sk-proj-...
#
# MODEL_PROVIDER=openai
# MODEL_NAME=gpt-4o                        # e.g. gpt-4o, gpt-4o-mini, gpt-4-turbo
# MODEL_API_KEY=sk-proj-...               # same value as OPENAI_API_KEY
#
# One-liner if you already have OPENAI_API_KEY set in your shell:
# MODEL_API_KEY=${OPENAI_API_KEY}

# ── Azure OpenAI ──────────────────────────────────────────────────────────────
# Only required if MODEL_PROVIDER=azure
# Native SDK env vars: AZURE_OPENAI_API_KEY, AZURE_OPENAI_ENDPOINT, AZURE_OPENAI_DEPLOYMENT_NAME
#
# MODEL_PROVIDER=azure
# MODEL_API_KEY=abc123def456...           # same value as AZURE_OPENAI_API_KEY
# MODEL_BASE_URL=https://my-resource.openai.azure.com/  # same value as AZURE_OPENAI_ENDPOINT
# AZURE_DEPLOYMENT=gpt-4o-prod           # same value as AZURE_OPENAI_DEPLOYMENT_NAME
# MODEL_NAME=gpt-4o                       # informational only — deployment name is used
#
# One-liner if you already have the Azure SDK vars set in your shell:
# MODEL_API_KEY=${AZURE_OPENAI_API_KEY}
# MODEL_BASE_URL=${AZURE_OPENAI_ENDPOINT}
# AZURE_DEPLOYMENT=${AZURE_OPENAI_DEPLOYMENT_NAME}

# ── AWS Bedrock ───────────────────────────────────────────────────────────────
# Only required if MODEL_PROVIDER=bedrock
# Native SDK env vars: AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, AWS_SESSION_TOKEN (optional)
# Credentials are resolved via the standard AWS SDK chain:
#   1. env vars  2. ~/.aws/credentials  3. instance profile / ECS task role
#
# MODEL_PROVIDER=bedrock
# MODEL_NAME=anthropic.claude-3-5-sonnet-20241022-v2:0  # Bedrock model ID
# AWS_REGION=us-east-1
#
# If using static credentials (not recommended for production):
# AWS_ACCESS_KEY_ID=AKIA...
# AWS_SECRET_ACCESS_KEY=wJalr...
# AWS_SESSION_TOKEN=AQoX...              # only needed for temporary credentials

# ── Google Gemini ─────────────────────────────────────────────────────────────
# Only required if MODEL_PROVIDER=gemini
# Native SDK env var: GOOGLE_API_KEY or GEMINI_API_KEY (AI Studio)
#
# MODEL_PROVIDER=gemini
# MODEL_NAME=gemini-1.5-pro              # e.g. gemini-1.5-pro, gemini-1.5-flash, gemini-2.0-flash
# MODEL_API_KEY=AIza...                  # same value as GOOGLE_API_KEY / GEMINI_API_KEY
#
# One-liner if you already have GOOGLE_API_KEY set in your shell:
# MODEL_API_KEY=${GOOGLE_API_KEY}

# ── Qdrant Vector Store ───────────────────────────────────────────────────────
QDRANT_HOST=localhost
QDRANT_PORT=6334
QDRANT_COLLECTION=tfai-docs
# QDRANT_API_KEY=  # Only needed for Qdrant Cloud

# ── Langfuse Observability ────────────────────────────────────────────────────
# LANGFUSE_HOST=http://localhost:3000
# LANGFUSE_PUBLIC_KEY=pk-...
# LANGFUSE_SECRET_KEY=sk-...
